{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e93177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configurations are done ... \n",
      "Adaptive prompt is ready ... \n",
      "Aggressive prompt is ready ... \n",
      "Happy prompt is ready ... \n",
      "Emotional prompt is ready ... \n",
      "Sarcastic prompt is ready ... \n",
      "Professional prompt is ready ... \n",
      "girlfriend prompt is ready ... \n",
      "boyfriend prompt is ready ... \n"
     ]
    }
   ],
   "source": [
    "# here are all the imports\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import gradio as gr\n",
    "\n",
    "%run ./config.ipynb # imports the configuration \n",
    "%run ./prompts.ipynb # imports all the prompt type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the history list where we'll store all the hoistory messages for next responces\n",
    "\n",
    "message_history = []\n",
    "\n",
    "UP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e957ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat phase is raday ...\n"
     ]
    }
   ],
   "source": [
    "def llama(user_prompt, prompt_type, clear):\n",
    "\n",
    "    UP.append(user_prompt)\n",
    "\n",
    "    if clear:\n",
    "        message_history.clear()\n",
    "        \n",
    "\n",
    "    if prompt_type == \"normal\":\n",
    "        system_prompt = normal_system_prompt\n",
    "    elif prompt_type == \"aggressive\":\n",
    "        system_prompt = aggressive_system_prompt\n",
    "    elif prompt_type == \"happy\":\n",
    "        system_prompt = happy_system_prompt\n",
    "    elif prompt_type == \"emotional\":\n",
    "        system_prompt = emotional_system_prompt\n",
    "    elif prompt_type == \"sarcastic\":\n",
    "        system_prompt = sarcastic_system_prompt\n",
    "    elif prompt_type == \"professional\":\n",
    "        system_prompt = professional_system_prompt\n",
    "    elif prompt_type == \"girlfriend\":\n",
    "        system_prompt = girlfriend_system_prompt\n",
    "    elif prompt_type == \"boyfriend\":    \n",
    "        system_prompt = boyfriend_system_prompt\n",
    "\n",
    "    message_history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    message_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "     # json formate\n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\":message_history,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    #making request to ollama\n",
    "    response = requests.post( \n",
    "        URL,\n",
    "        headers = headers,\n",
    "        data = json.dumps(data)\n",
    "    )\n",
    "\n",
    "    # result stores the meaasge\n",
    "    result = response.json()[\"choices\"][0][\"message\"][\"content\"]  \n",
    "\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "    # returns the message given by llama\n",
    "    return result\n",
    "\n",
    "print(\"chat phase is raday ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e7377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
